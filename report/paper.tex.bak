\documentclass[journal, a4paper]{IEEEtran}

% some very useful LaTeX packages include:

%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

\usepackage{url}        % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}    % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
%\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/



% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 %----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

~\\[1cm]
\includegraphics{SCUT.png}\\[2cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[1cm]
{ \huge \bfseries The Experiment Report of \textit{Machine Learning} }\\[0.6cm] % Title of your document
\HRule \\[2cm]
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------


\textsc{\LARGE \textbf{School:} School of Software Engineering}\\[1cm]
\textsc{\LARGE \textbf{Subject:} Software Engineering}\\[2cm]


%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Da Ren % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Qingyao Wu % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]
~
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student ID:}\\
201720144900
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Grade:} \\
Graduate
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise


%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

% Define document title and author
	\title{Logistic Regression, Linear Classification and Stochastic Gradient Descent}
	\maketitle

% Write abstract here
\begin{abstract}
Gradient descent plays an important role in machine learning. It can help us get the global optimal value of a function. However, one of the most important task in machine learning is to get the optimal value for loss function. Gradient descent is widely used in this task. However, people gradually find that traditional gradient descent performs not well when dealing with some tasks. Therefore, there are many modified gradient descent methods which are proposed. I implement NAG, RMSProp, AdaDelta and Adam in logistic regression and linear regression in this experiment.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
	% \PARstart{}{} creates a tall first letter for this first paragraph
\PARstart{G}{radient} descent is a important method in machine learning. The most important tasks in many machine learning models is to update or calculate their parameters which can lead the corresponding loss functions value to be minimal value. Although gradient descent is widely used, researchers find that it has its own limitation. Therefore, there are many modified gradient descent methods which are proposed, \emph{e.g}. NAG, RMSProp, AdaDelta.

% Main Part
\section{Methods and Theory}
Gradient descent is flexible since it can be easily used in different kinds of loss function. It is based on the observation that if the multi-variable function $F(x)$ is defined and differentiable in a neighborhood of a point $a$, then $F(x)$ decreases fastest if one goes from $a$ in the direction of the negative gradient of $F$ at $a$. Therefore, if we want to minimize the value of $F(x)$, we can move $a$ against the gradient of $F$. Therefore, $a$ can be updated by Eq.~\eqref{eq1}.
\begin{equation}
a_{t+1}=a_{t} + \lambda \Delta F(a_{t}) \label{eq1}
\end{equation}
where $a_{t}$ is the value of $a$ at time $t$, $\lambda$ is the learning rate which is always set to be a small value like 0.1. $\Delta F(a_{t})$ is the gradient value of $F(a_{t})$.

There are two experiments in my work. One is linear regression, the other is linear classification. Therefore I have to choose different loss function to different tasks.

Linear regression is a linear approach for modeling the relationship between a scalar dependent variable $y$ and one or more explanatory variables denoted $X$. It can be calculated as following
\begin{equation}
y' = W^TX+b \label{eq2}
\end{equation}
where $W$ is the weight matrix, and $b$ is the bias variable. However, we can add a new column to $X$ to be $X'$ where the column is all 1. Therefore, we can not use $b$ anymore, since the weight of the all 1 column can act the role of $b$. $y$ can be calculated as following
\begin{equation}
y' = W^TX' \label{eq3}
\end{equation}


Then, I choose the loss function which is described in Eq.~\eqref{eq4}
\begin{equation}
L_1 = 1/{2m} \sum_{i=1}^m(y'_i-y_i)^2 \label{eq4}
\end{equation}
where $m$ is the total number of data and $y_i$ is the value of training data. $y'_i$ is the value which is calculated by linear model. To use the gradient descent to optimize the model, we have to calculate the derivation of Eq.~\eqref{eq4}.
\begin{equation}
\Delta L_1 / \Delta w = 1/(2m) \sum_{i=1}^m 2(y'_i-y_i)x_i = 1/m \sum_{i=1}^m (y'_i-y_i)x_i
\label{eq5}
\end{equation}

Now, we can use Eq.~\eqref{eq5} to calculate derivative at each step and update weight in Eq.~\eqref{eq1}. After several iterations with a suitable learning rate, we can get a weight matrix which can lead the loss function's value into global minimal.

In the linear classification task, I also use the Eq.~\eqref{eq3}. Since it is a different task, I choose another loss function. The loss function I choose is called hinge loss, which can be described in Eq.~\eqref{eq6}
\begin{equation}
L_2 = \begin{Vmatrix}W\end{Vmatrix}^2/2 + C/m \sum_{i=1}^m max(0, 1-y_i(W^Tx_i+b))
\label{eq6}
\end{equation}
where $C$ is a hyperparameter which can be adjusted in different dataset.

To use the gradient descent to optimize the weight matrix, we have to calculate the the derivation of Eq.~\eqref{eq6}.
\begin{equation}
\Delta L_2 / \Delta w = W + C/m\sum_{i=1}^m g_w(x_i)
\label{eq7}
\end{equation}
where
\begin{equation}
g_w(x_i)=\begin{cases}
-y_ix_i\quad 1-y_i(W^Tx'_i) \geq 0 \\
0\quad \quad \quad 1-y_i(W^Tx'_i) < 0 \\
\end{cases}
\label{eq8}
\end{equation}

We can update the linear classification according to Eq.~\eqref{eq1} and use Eq.~\eqref{eq8} to calculate the derivative.

After choosing the loss function and calculate their corresponding derivation, I can use gradient descent to train these two models. My experiment will be described in following.

\section{Experiments}
There will be two experiments which will be introduced. One is training the linear regression and the other is training the linear classification. Both of them is trained in gradient descent.

\subsection{Dataset}
This is two different tasks, so we have to use two different datasets. To train the linear regression model, I use the housing data in LIBSVM Data. There are 506 data and each of them has 13 attributes. For the linear classification tasks, I use the australian data in LIBSVM Data. There are 690 data and each of them has 14 attributes. All the datasets I use is the scaled version.

\subsection{Implementation}
In this section, I will introduce the linear regression experiment and linear classification experiment in detail.

In the linear regression experiment, the learning rate $\lambda$ is set to be 0.1. The weight matrix is initialized randomly. After I read the data from file, I split the dataset into training set and validation set. There are 354 data in training set and 152 data in validation set. The iteration's number is set to 300. The loss value of training set and validation set is described in Fig.~\ref{fig_1}.
\begin{figure}[htbp]
\centerline{\includegraphics[height=150px]{line_1.png}}
\caption{The loss value of training set and test set in linear regression.}
\label{fig_1}
\end{figure}

We can find that both of the loss of training set and test set decrease quickly at the beginning and close to unchange at the end of iterations. It means that the value of loss function is closed to global minimal value.

In the linear classfication experiment, the learning rate $\lambda$ is set to be 0.1. The weight matrix is initialized randomly. I split the dataset into training set and validation set. There are 483 data in training set and 207 data in validation set. The iteration's number is set to 300. The loss value of training set and validation set is described in Fig.~\ref{fig_2}.

\begin{figure}[htbp]
\centerline{\includegraphics[height=150px]{line_2.png}}
\caption{The loss value of training set and test set in linear classification.}
\label{fig_2}
\end{figure}

We can find that both of the loss of training set and test set decrease quickly at the beginning and close to unchange at the end of iterations. It means that the value of loss function is closed to global minimal value.

After finish training, I set a threshold to classify the data in validation set. I set the threshold to be 0, if the value calculated by model is smaller than 0, the corresponding data will be classified into one class (the label of this class is -1). If the value is larger or equal than 0, the corresponding data will be classified into another class (the label of this class is 1). And then I calculate the precision, recall and f1-score which is shown in Table~\ref{table_1}.

    \begin{table}[!hbt]
		% Center the table
		\begin{center}
		% Title of the table
		\caption{Classification Results}
		\label{table_1}
		% Table itself: here we have two columns which are centered and have lines to the left, right and in the middle: |c|c|
		\begin{tabular}{|c|c|c|c|c|}
			% To create a horizontal line, type \hline
			\hline
			% To end a column type &
			% For a linebreak type \\
			  & precision & recall & f1-score & support \\
			\hline
			class1 & 0.93 & 0.81 & 0.87  & 117 \\
			\hline
			class2 & 0.79 & 0.92 & 0.85  & 90 \\
			\hline
			avg / total & 0.87 & 0.86 & 0.86 & 207 \\
			\hline
		\end{tabular}
		\end{center}
	\end{table}

From Table~\ref{table_1}, we can find that the precision, recall and f1-score of classification is larger than 0.85. It is a good result of linear classification.

\section{Conclusion}
In this experiment, I implement gradient descent in linear regression and linear classification. Both of their weight matrix is updated so that corresponding loss functions can be minimized. However, the loss function I use is convex, so that I don't need to care about the problem of local minimal. If the loss function is non-convex, gradient descent maybe fail to find the global minimal. Learning how to use gradient descent to get global minimal in non-convex function is my future work.

% Your document ends here!
\end{document}
